# Token Counter API Product Specification

## Overview

The Token Counter API is a FastAPI-based Python application that provides a REST API to count tokens in a given text input. This tool helps anticipate token usage for AI-powered features and can be integrated into different environments.

## Goals & Objectives

- Accept text inputs via an API endpoint
- Return the count of tokens required to process the input
- Maintain environment-specific configurations through environment variables (e.g., dev with mock data, qa and prod with real data)
- Provide short, efficient responses for easy downstream consumption
- Support tokenization specific to OpenAI models

## Key Features

### Token Counting Endpoint
- Receives text input and returns estimated token count
- Supports specifying the tokenization method/model (primarily OpenAI models: gpt-3.5-turbo, gpt-4, text-davinci-003)
- Offers batch processing capability for multiple text inputs

### Environment-Specific Handling
- Dev environment: Mock data or logging allowed
- QA & Prod: Use real data without stubbing or mock data

### Error Handling
- Return meaningful HTTP status codes and messages for invalid inputs
- Include detailed error information to aid debugging and integration

### Security
- Basic authentication for API access
- Rate limiting to prevent abuse

## Architecture & Implementation

### Framework
- FastAPI in Python (3.9+) in python venv

### Endpoints
- `POST /v1/tokens/count` – Accepts text input and returns the estimated token count
- `POST /v1/tokens/batch-count` – Accepts multiple text inputs and returns count for each
- `GET /v1/health` – Provides a health check
- `GET /docs` – OpenAPI documentation

### Data Flow
1. Client sends text to `POST /v1/tokens/count`
2. Service calculates tokens using OpenAI's tiktoken library for accurate tokenization
3. The service responds with a JSON object containing token count and metadata

### Response Format
```json
{
  "text_id": "optional-client-reference-id",
  "token_count": 125,
  "model": "gpt-3.5-turbo",
  "processing_time_ms": 5
}
```

### Configuration Management
- Use environment variables to switch behaviors for dev (mock allowed), qa, and prod
- No stubbing or fake data for qa and prod
- Configuration for supported models and their tokenization methods

## Performance Expectations
- Average response time under 100ms for texts up to 10,000 characters
- Support for concurrent requests (at least 100 requests per second)

## Testing

- **Unit Tests**: Validate token counting logic against sample inputs
- **Integration Tests**: Ensure endpoints function and environment variable switching works
- **Load Tests**: Verify performance under expected load conditions
- **Benchmark Tests**: Compare tokenization methods for accuracy and performance

## Deployment & Maintenance

### Deployment
- Maintain separate configurations for dev, qa, and prod (in .env files if applicable)
- Use Docker or other deployment methods if needed
- CI/CD pipeline for automated testing and deployment

### Versioning
- API versioning strategy (e.g., `/v1/tokens/count`) for future updates
- Deprecation policy for outdated endpoints or features

### Monitoring
- Log token counts and any service errors
- Setup alerts for spikes in usage or high error rates
- Prometheus metrics for performance monitoring

## Examples

### Sample Request
```bash
curl -X POST "https://api.example.com/v1/tokens/count" \
     -H "Content-Type: application/json" \
     -d '{"text": "Hello world!", "model": "gpt-3.5-turbo"}'
```

### Sample Response
```json
{
  "token_count": 3,
  "model": "gpt-3.5-turbo",
  "processing_time_ms": 2
}
```

<!-- Generated by Copilot -->
